#see also https://cromwell.readthedocs.io/en/stable/backends/SLURM/
# include the application.conf at the top
include required(classpath("application"))
#removed -p ${queue}
workflow-options {
  workflow-failure-mode = ContinueWhilePossible
  delete_intermediate_output_files = true
  final_workflow_outputs_dir = "cromwell-results",
  use_relative_output_paths = true,
  final_workflow_log_dir = "cromwell-logs",
  final_call_logs_dir = "cromwell-call_logs",
  memory_retry_multiplier = 1.1
}
#database {
#  profile = "slick.jdbc.HsqldbProfile$"
#  db {
#    driver = "org.hsqldb.jdbcDriver"
#    url = """
#    jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;
#    shutdown=false;
#    hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
#    hsqldb.result_max_memory_rows=50000;
#    hsqldb.large_data=true;
#    hsqldb.applog=1;
#    hsqldb.lob_compressed=true;
#    hsqldb.script_format=3
#    """
#    connectionTimeout = 120000
#    numThreads = 1
#   }
#}
system {
  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  #abort-jobs-on-terminate = false

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  #graceful-server-shutdown = true

  # Cromwell will cap the number of running workflows at N
  #max-concurrent-workflows = 5000

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  #max-workflow-launch-count = 50

  # Number of seconds between workflow launches
  #new-workflow-poll-rate = 20

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  #number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  #number-of-cache-read-workers = 25

  io {
    # throttle {
    # # Global Throttling - This is mostly useful for GCS and can be adjusted to match
    # # the quota availble on the GCS API
    # #number-of-requests = 100000
    # #per = 100 seconds
    # }

    # Number of times an I/O operation should be attempted before giving up and failing it.
    #number-of-attempts = 5
  }

  # Maximum number of input file bytes allowed in order to read each type.
  # If exceeded a FileSizeTooBig exception will be thrown.
  input-read-limits {

    #lines = 128000

    #bool = 7

    #int = 19

    #float = 50

    #string = 128000

    #json = 128000

    #tsv = 128000

    #map = 128000

    #object = 128000
  }

  abort {
    # These are the default values in Cromwell, in most circumstances there should not be a need to change them.

    # How frequently Cromwell should scan for aborts.
    scan-frequency: 30 seconds

    # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.
    # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask
    # the associated WorkflowActor to abort again.
    cache {
      enabled: true
      # Guava cache concurrency.
      concurrency: 1
      # How long entries in the cache should live from the time they are added to the cache.
      ttl: 20 minutes
      # Maximum number of entries in the cache.
      size: 100000
    }
    #memory retry strings
    memory-retry-error-keys = ["OutOfMemory", "Killed"]
  }

  # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration
  dns-cache-ttl: 3 minutes
}
backend {
  #default = "Local"
  default = "Slurm"
  providers {
    Slurm {
        temporary-directory = "$($$TMPDIR/)"
        actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
        config {
          #<10 for testing ~200 for running?
          concurrent-job-limit = 200
          #concurrent-job-limit = 10
          script-epilogue = ""
          runtime-attributes = """
            Int timeMinutes = 960
            Int cpus = 1
            Int memory = 1024
            String? docker
            String? docker_user
            Int disk = 50
          """
          #exclude quirky nodes like this
          #            --exclude=node95 \
          submit = """
            perl -i.bak -wpe 's/^(tmpDir=.*)/$1;source \$HOME\/.bashrc;if [ "x\$TMPDIR" != "x" ]; then tmpdir="\$TMPDIR";fi;echo "\$tmpDir"/g' ${script} && sleep 15s &&
            sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} \
            -t ${timeMinutes}  \
            ${"--cpus-per-task=" + cpus} \
            --get-user-env=30L \
            --mem=${memory} \
            --partition=regular \
            --tmp=${disk} \
            --wrap "/usr/bin/env bash ${script}"
          """
          #submit = """
          #  sleep 1m &&
          #  sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} \
          # -t ${timeMinutes}  \
          #  ${"--cpus-per-task=" + cpus} --get-user-env=30L \
          #  --mem=${memory} \
          #  --tmp=${disk} \
          #  --wrap "/usr/bin/env bash ${script}"
          #"""

          kill = "scancel ${job_id}"
          check-alive = "squeue -j ${job_id}"
          job-id-regex = "Submitted batch job (\\d+).*"
        
          filesystems {
            local {
              localization: [
                "hard-link", "soft-link", "copy"
              ]
            }
          }
          default-runtime-attributes {
            failOnStderr: false
            continueOnReturnCode: 0
            maxRetries: 3
          }
        } 
    },
    Local {
      # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend.
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      # The backend custom configuration.
      config {

        # Optional limits on the number of concurrent jobs
        concurrent-job-limit = 4

        # If true submits scripts to the bash background using "&". Only usefull for dispatchers that do NOT submit
        # the job and then immediately return a scheduled job id.
        run-in-background = true

        # `temporary-directory` creates the temporary directory for commands.
        #
        # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:
        # temporary-directory = "$(mktemp -d \"$PWD\"/tmp.XXXXXX)"
        #
        # The expression is run from the execution directory for the script. The expression must create the directory
        # if it does not exist, and then return the full path to the directory.
        #
        # To create and return a non-random temporary directory, use something like:
        # temporary-directory = "$(mkdir -p /tmp/mydir && echo /tmp/mydir)"
        # script-epilogue
        #
        # `script-epilogue` configures a shell command to run after the execution of every command block.
        #
        # If this value is not set explicitly, the default value is `sync`, equivalent to:
        # script-epilogue = "sync"
        #
        # To turn off the default `sync` behavior set this value to an empty string:
        # script-epilogue = ""

        # `glob-link-command` specifies command used to link glob outputs, by default using hard-links.
        # If filesystem doesn't allow hard-links (e.g., beeGFS), change to soft-links as follows:
        # glob-link-command = "ln -sL GLOB_PATTERN GLOB_DIRECTORY"

        # The list of possible runtime custom attributes.
        runtime-attributes = """
        String? docker
        String? docker_user
        """

        # Submit string when there is no "docker" runtime attribute.
        submit = "/usr/bin/env bash ${script}"

        # Submit string when there is a "docker" runtime attribute.
        submit-docker = """
        docker run \
          --rm -i \
          ${"--user " + docker_user} \
          --entrypoint ${job_shell} \
          -v ${cwd}:${docker_cwd} \
          ${docker} ${docker_script}
        """

        # Root directory where Cromwell writes job results.  This directory must be
        # visible and writeable by the Cromwell process as well as the jobs that Cromwell
        # launches.
        root = "cromwell-executions"

        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above.
        dockerRoot = "/cromwell-executions"

        # File system configuration.
        filesystems {

          # For SFS backends, the "local" configuration specifies how files are handled.
          local {

            # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files.
            localization: [
              "hard-link", "soft-link", "copy"
            ]
            # An experimental localization strategy called "cached-copy" is also available for SFS backends.
            # This will copy a file to a cache and then hard-link from the cache. It will copy the file to the cache again
            # when the maximum number of hardlinks for a file is reached. The maximum number of hardlinks can be set with:
            # max-hardlinks: 950

            # Call caching strategies
            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "hard-link", "soft-link", "copy"
              ]

              # Possible values: file, path, path+modtime
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
              # Default: file
              hashing-strategy: "file"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
            }
          }
        }

        # The defaults for runtime attributes if not provided.
        default-runtime-attributes {
          failOnStderr: false
          continueOnReturnCode: 0
          maxRetries: 3
        }
      }
    }
  }
}
